{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce72af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhi/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/siddhi/miniconda3/envs/mistral-qlora/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/siddhi/miniconda3/envs/mistral-qlora/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/siddhi/miniconda3/envs/mistral-qlora/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61069614",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/data3/ritika_project/mistral-7b\"\n",
    "\n",
    "# The name for our new, fine-tuned model adapter\n",
    "new_model_name = \"mistral-7b-english-to-french\"\n",
    "output_base_dir = \"/data3/ritika/mistral_training_output\"\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_base_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0083bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model from: /data3/ritika_project/mistral-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████| 2/2 [00:24<00:00, 12.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading base model from: {model_id}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" \n",
    "print(\"Base model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83cbe60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prepared for QLoRA training.\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"Model prepared for QLoRA training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed53de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_translation_dataset(en_path, fr_path):\n",
    "    \"\"\"\n",
    "    Loads parallel data assuming line-by-line alignment between the two files.\n",
    "    This is the corrected version for files without utterance IDs.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(en_path):\n",
    "        raise FileNotFoundError(f\"English data file not found at: {en_path}\")\n",
    "    if not os.path.exists(fr_path):\n",
    "        raise FileNotFoundError(f\"French data file not found at: {fr_path}\")\n",
    "\n",
    "    print(f\"Reading English file from {en_path}...\")\n",
    "    with open(en_path, 'r', encoding='utf-8') as f:\n",
    "        en_lines = [line.strip() for line in f]\n",
    "\n",
    "    print(f\"Reading French file from {fr_path}...\")\n",
    "    with open(fr_path, 'r', encoding='utf-8') as f:\n",
    "        fr_lines = [line.strip() for line in f]\n",
    "\n",
    "    if len(en_lines) != len(fr_lines):\n",
    "        raise ValueError(\n",
    "            f\"The number of lines in the English and French files do not match. \"\n",
    "            f\"English: {len(en_lines)}, French: {len(fr_lines)}\"\n",
    "        )\n",
    "\n",
    "    data = []\n",
    "    print(f\"Creating {len(en_lines)} instruction pairs...\")\n",
    "    for i in range(len(en_lines)):\n",
    "        en_text = en_lines[i]\n",
    "        fr_text = fr_lines[i]\n",
    "        # Ensure that we don't process empty lines\n",
    "        if en_text and fr_text:\n",
    "            data.append({\n",
    "                \"instruction\": en_text,\n",
    "                \"response\": fr_text\n",
    "            })\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "396624cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading English file from /data3/ritika/data/raw/train_100h_txt/train/train.en...\n",
      "Reading French file from /data3/ritika/data/raw/train_100h_txt/train/train.fr...\n",
      "Creating 47271 instruction pairs...\n",
      "Loaded 47271 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████| 47271/47271 [00:06<00:00, 7664.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def create_prompt_format(sample):\n",
    "    \"\"\"Creates a formatted prompt string from a dataset sample.\"\"\"\n",
    "    return f\"English:\\n{sample['instruction']}\\nFrench:\\n{sample['response']}\"\n",
    "\n",
    "# *** MAKE SURE THESE PATHS ARE CORRECT FOR YOUR SYSTEM ***\n",
    "train_en_path = \"/data3/ritika/data/raw/train_100h_txt/train/train.en\"\n",
    "train_fr_path = \"/data3/ritika/data/raw/train_100h_txt/train/train.fr\"\n",
    "\n",
    "# Load the data\n",
    "training_data = load_translation_dataset(train_en_path, train_fr_path)\n",
    "\n",
    "# For testing, you might want to use a smaller subset of your data\n",
    "# training_data = training_data[:1000] # Uncomment to use only the first 1000 samples\n",
    "\n",
    "print(f\"Loaded {len(training_data)} samples.\")\n",
    "\n",
    "# Convert the list of dictionaries to a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(training_data)\n",
    "\n",
    "# Create the final formatted dataset with a 'text' column\n",
    "formatted_dataset = dataset.map(lambda sample: {'text': create_prompt_format(sample)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "929fe6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'ADIEU VALENTINE ADIEU', 'response': '--Adieu, Valentine, adieu!'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e79d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'ADIEU VALENTINE ADIEU', 'response': '--Adieu, Valentine, adieu!', 'text': 'English:\\nADIEU VALENTINE ADIEU\\nFrench:\\n--Adieu, Valentine, adieu!'}\n"
     ]
    }
   ],
   "source": [
    "print(formatted_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a88a44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████| 47271/47271 [01:47<00:00, 437.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{output_base_dir}/results_translation\",\n",
    "    num_train_epochs=3, # Training for 1 full epoch is a good starting point\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=1, # Accumulate gradients over 4 steps\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10, \n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=128, \n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef72be8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on the custom translation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhi/miniconda3/envs/mistral-qlora/lib/python3.10/site-packages/transformers/trainer.py:2553: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "/home/siddhi/miniconda3/envs/mistral-qlora/lib/python3.10/site-packages/transformers/trainer.py:2305: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/siddhi/miniconda3/envs/mistral-qlora/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2346' max='4434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2346/4434 44:34 < 10:46:26, 0.05 it/s, Epoch 1.59/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>1.262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>1.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>1.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>1.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>1.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>1.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>1.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>1.635100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>1.283800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>1.185800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>1.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>1.657500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Starting training on the custom translation dataset...\")\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = f\"{output_base_dir}/{new_model_name}\"\n",
    "print(f\"Saving fine-tuned model adapter to {final_model_path}\")\n",
    "# *** BEST PRACTICE: Use safe_serialization to save in the better .safetensors format ***\n",
    "trainer.model.save_pretrained(final_model_path, safe_serialization=True)\n",
    "print(\"Model adapter saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
